# SS22-Bachelorarbeit-Recommender
## Models
The models are placed and defined in `model_workspace` and configured in Experiment models in the file `model_config.py` 
in the folder `utils` from which the test models and respective data and batchers will be loaded from.

The execution of the test models and data collecting for the experimental/analytical part of the bachelor thesis is
situated in the file `run_models.py` and the methods for GNN models is situated in `run_tools.py`.
## Utils
There are several util python files which are used for defining, executing and testing the models. It splits in 3 parts:
### `model_config.py`
Contains as already explained the configuration of the test models and the settings and also the methods for loading
the data.
### `accuracy`
Contains different types of python files for different aspects of accuracy:
- `accuracy_bpr.py` contains an alternative loss which is said to be optimized for training precision and recall scores
- `accuracy_oversmoothing.py` contains a method for computing the MAD score which is representative for the 
oversmoothing of the model
- `accuracy_recommender.py` contains the accuracy functions for GNN model from the pytorch library. Contained accuracy
functions calculate the ROC curve and the precision and recall accuracy scores
- `accuracy_surpriselib.py` contains the accuracy functions for baseline recommender from surpriselib. Contained 
accuracy functions calculate the ROC curve and the precision and recall accuracy scores.
- `stopping_control.py` contains a class with logic that stores roc auc values of validation data and determines if the
training process should be stopped or not.
### `data_related`
Contains helper libraries working or generating data.
- `data_gen.py` contains function that load the data loading or creating functions. If the pre-generated x files already
exist then they will be loaded, instead of loading the smiles data from the dataset and generating the x data with it.
Not recommended as it takes up to 1h52m to finish.
- `data_protocoller.py` contains a class that will be given all trackworthy data while running the test model training
and testing and then saving and/or printing it to console
- `edge_batch.py` contains the Batcher class which splits the edges in subsets and executes neighbor samples for the
subset of edges to make convolution layer possible
- `x_data_transform.py` contains a function that takes the raw chemical descriptor values as input and remove empty
features, shift features containing negative features to positive features, scale values to an interval 0 to 1 and
handle very large entries (>10^10) by either removing them if there are too few or scale them with logarithm before
scaling it to 0 to 1.
## Data placement
As the data necessary is too large for GitHub please use following guidelines to position the csv data files in the file
structure of the repo when cloned:
- df_assay_entries.csv should be positioned in the top layer of the repo (in the same level as run_models.py)
- csv containing the x-data (currently descriptors_x_transformed2.csv) should be positioned in a folder named "data". 
Theoretically the data could be generated by the function in data_gen.py but it takes 1h52min to produce the data and 
the data is still in its raw state as the processing is only present in a Notepad.

The csv files in data can be downloaded as a zip (which should already create the folder data) via the following link: 
https://ucloud.univie.ac.at/index.php/s/BoaDYO7CxkTx5GI. Please unpack this data containing the following files:
- descriptors_x.csv containing the unmodified chemical Descriptors
- descriptors_x_transformed2.csv containing the new transformed x data which will be used as node features if chemical
Data is loaded.